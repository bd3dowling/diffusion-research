{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses more of library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Callable\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro.distributions as dist\n",
    "from IPython.display import HTML\n",
    "from jax import grad, vmap\n",
    "from jax.tree_util import Partial\n",
    "from jaxtyping import Array, PRNGKeyArray, Real\n",
    "\n",
    "from diffusionlib.sampler import SamplerName, get_sampler\n",
    "from diffusionlib.sde import SDE, VP\n",
    "from diffusionlib.smc.feynman_kac_model import LikelihoodGuidedPF\n",
    "from diffusionlib.smc.state_space_model import SimpleObservationSSM\n",
    "\n",
    "matplotlib.rcParams[\"animation.embed_limit\"] = 2**128\n",
    "COLOR_POSTERIOR = \"#a2c4c9\"\n",
    "COLOR_ALGORITHM = \"#ff7878\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at an example of a 2 ($dim_x$) dimensional GMM with 25, evenly spaced clusters centered\n",
    "on means $(8 \\times \\{-2, ..., 2\\}, 8 \\times \\{-2, ..., 2\\})$ with unit covariances. Our observation\n",
    "is some 2 ($dim_y$) dimensional value using which we'd like to be able to sample from the posterior\n",
    "distribution. The config below is easily changeable, and empirically the below methodology has\n",
    "worked well with more (or fewer) clusters, larger $dim_x$, and larger $dim_y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "key = random.PRNGKey(100)\n",
    "num_steps = 1000\n",
    "num_samples = 1000\n",
    "dim_x = 2\n",
    "dim_y = 2\n",
    "measurement_noise_std = 1.0\n",
    "size = 8.0  # mean multiplier\n",
    "center_range = np.array((-2, 2))\n",
    "\n",
    "beta_min = 0.01\n",
    "beta_max = 20.0\n",
    "\n",
    "# plotting\n",
    "chart_lims = size * (center_range + np.array((-0.5, 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [MCGDiff paper](https://arxiv.org/pdf/2308.07983.pdf) (page 5), the marginals of the\n",
    "forward process are available in closed form, meaning we don't need a trained model to\n",
    "estimate the score or noise ($\\epsilon$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epsilon functions\n",
    "# NOTE: model/epsilon function is just the negative score by the variance\n",
    "def get_model_fn(\n",
    "    ou_dist: Callable[[Array], dist.MixtureSameFamily], sde: SDE\n",
    ") -> Callable[[Array, Array], Array]:\n",
    "    return vmap(\n",
    "        grad(\n",
    "            lambda x, t: -jnp.sqrt(sde.marginal_variance(t))\n",
    "            * ou_dist(sde.marginal_mean_coeff(t)).log_prob(x)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the mixture model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mixture model function\n",
    "def ou_mixt(mean_coeff: float, means: Array, dim_x: int, weights: Array) -> dist.MixtureSameFamily:\n",
    "    means = jnp.vstack(means) * mean_coeff\n",
    "    covs = jnp.repeat(jnp.eye(dim_x)[None], axis=0, repeats=means.shape[0])\n",
    "    return dist.MixtureSameFamily(\n",
    "        component_distribution=dist.MultivariateNormal(loc=means, covariance_matrix=covs),\n",
    "        mixing_distribution=dist.CategoricalProbs(weights),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a helper function for Gaussian-Gaussian posterior calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define posterior\n",
    "def gaussian_posterior(\n",
    "    y: Real[Array, \" dim_y\"],\n",
    "    likelihood_a: Real[Array, \"dim_y dim_x\"],\n",
    "    likelihood_bias: Real[Array, \" dim_y\"],\n",
    "    likelihood_precision: Real[Array, \"dim_y dim_y\"],\n",
    "    prior_loc: Real[Array, \" dim_x\"],\n",
    "    prior_covar: Real[Array, \"dim_x dim_x\"],\n",
    ") -> dist.MultivariateNormal:\n",
    "    # Compute the precision matrix of the prior distribution\n",
    "    prior_precision_matrix = jnp.linalg.inv(prior_covar)\n",
    "\n",
    "    # Calculate the precision matrix of the posterior distribution\n",
    "    posterior_precision_matrix = (\n",
    "        prior_precision_matrix + likelihood_a.T @ likelihood_precision @ likelihood_a\n",
    "    )\n",
    "\n",
    "    # Calculate the covariance matrix of the posterior distribution\n",
    "    posterior_covariance_matrix = jnp.linalg.inv(posterior_precision_matrix)\n",
    "\n",
    "    # Calculate the mean of the posterior distribution\n",
    "    posterior_mean = posterior_covariance_matrix @ (\n",
    "        likelihood_a.T @ likelihood_precision @ (y - likelihood_bias)\n",
    "        + prior_precision_matrix @ prior_loc\n",
    "    )\n",
    "\n",
    "    # Ensure symmetry and numerical stability of the covariance matrix\n",
    "    # Handle potential numerical issues by regularization\n",
    "    try:\n",
    "        posterior_covariance_matrix = (\n",
    "            posterior_covariance_matrix + posterior_covariance_matrix.T\n",
    "        ) / 2\n",
    "    except ValueError:\n",
    "        u, s, v = jnp.linalg.svd(posterior_covariance_matrix, full_matrices=False)\n",
    "        s = jnp.clip(s, 1e-12, 1e6).real\n",
    "        posterior_covariance_matrix = u.real @ jnp.diag(s) @ v.real\n",
    "        posterior_covariance_matrix = (\n",
    "            posterior_covariance_matrix + posterior_covariance_matrix.T\n",
    "        ) / 2\n",
    "\n",
    "    return dist.MultivariateNormal(\n",
    "        loc=posterior_mean, covariance_matrix=posterior_covariance_matrix\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function which provides essentially exact posterior samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define posterior for the mixture model\n",
    "def get_posterior(\n",
    "    obs: Array, prior: dist.MixtureSameFamily, a: Array, sigma_y: Array\n",
    ") -> dist.MixtureSameFamily:\n",
    "    mixing_dist: dist.CategoricalProbs = prior.mixing_distribution\n",
    "    component_dist: dist.MultivariateNormal = prior.component_distribution  # type: ignore\n",
    "    comp_mean = component_dist.mean\n",
    "    comp_cov: Array = component_dist.covariance_matrix  # type: ignore\n",
    "\n",
    "    # Precompute the inverse of the observation noise covariance matrix\n",
    "    precision = jnp.linalg.inv(sigma_y)\n",
    "    modified_means = []\n",
    "    modified_covars = []\n",
    "    weights = []\n",
    "\n",
    "    # Iterate through the components of the prior distribution\n",
    "    for loc, cov, weight in zip(comp_mean, comp_cov, mixing_dist.probs):\n",
    "        # Compute the posterior distribution for the current component\n",
    "        new_dist = gaussian_posterior(obs, a, jnp.zeros_like(obs), precision, loc, cov)\n",
    "        modified_means.append(new_dist.mean)\n",
    "        modified_covars.append(new_dist.covariance_matrix)\n",
    "\n",
    "        # Calculate the prior likelihood and residual\n",
    "        prior_x = dist.MultivariateNormal(loc, covariance_matrix=cov)\n",
    "        residue = obs - a @ new_dist.loc\n",
    "\n",
    "        # Compute log-probability contributions\n",
    "        log_constant = (\n",
    "            -0.5 * residue @ precision @ residue.T\n",
    "            + prior_x.log_prob(new_dist.mean)\n",
    "            - new_dist.log_prob(new_dist.mean)\n",
    "        )\n",
    "\n",
    "        # Compute the log weight for the component\n",
    "        weights.append(jnp.log(weight) + log_constant)\n",
    "\n",
    "    # Normalize weights\n",
    "    weights = jnp.array(weights)\n",
    "    normalized_weights = weights - jax.scipy.special.logsumexp(weights)\n",
    "\n",
    "    # Construct categorical distribution from the normalized weights\n",
    "    categorical_distribution = dist.CategoricalLogits(logits=normalized_weights)\n",
    "\n",
    "    # Construct a mixture distribution of multivariate normals\n",
    "    multivariate_mixture = dist.MultivariateNormal(\n",
    "        loc=jnp.stack(modified_means, axis=0),\n",
    "        covariance_matrix=jnp.stack(modified_covars, axis=0),\n",
    "    )\n",
    "\n",
    "    return dist.MixtureSameFamily(categorical_distribution, multivariate_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define functions for the inverse problem; see\n",
    "[MCGDiff paper](https://arxiv.org/pdf/2308.07983.pdf), Appendix B.3.1 for details on the measurement \n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse problem functions\n",
    "def extended_svd(a: Array) -> tuple[Array, Array, Array, Array]:\n",
    "    # Compute the singular value decomposition\n",
    "    u, s, v = jnp.linalg.svd(a, full_matrices=False)\n",
    "\n",
    "    # Create a coordinate mask based on the length of the singular values\n",
    "    coordinate_mask = jnp.concatenate([jnp.ones(len(s)), jnp.zeros(v.shape[0] - len(s))]).astype(\n",
    "        bool\n",
    "    )\n",
    "\n",
    "    return u, s, v, coordinate_mask\n",
    "\n",
    "\n",
    "def generate_measurement_equations(\n",
    "    dim_x: int,\n",
    "    dim_y: int,\n",
    "    mixt: dist.MixtureSameFamily,\n",
    "    noise_std: float,\n",
    "    key: PRNGKeyArray,\n",
    "):\n",
    "    # Generate random keys for different sources of randomness\n",
    "    key_a, key_diag, key_init_sample, key_init_obs = random.split(key, 4)\n",
    "\n",
    "    # Create random matrix\n",
    "    a = random.normal(key_a, (dim_y, dim_x))\n",
    "\n",
    "    # Build extended SVD\n",
    "    u, s, v, coordinate_mask = extended_svd(a)\n",
    "\n",
    "    # Re-create `s` using uniform sampling, sorting the generated values to align with\n",
    "    # properties of singular values being ordered in the SVD Sigma (`s`) matrix\n",
    "    s_new = jnp.sort(random.uniform(key_diag, s.shape), descending=True)\n",
    "    s_new_mat = jnp.diag(s_new)\n",
    "\n",
    "    # Re-construct `a` using the sorted diag and coordinate mask\n",
    "    a_recon: Real[Array, \"{dim_y} {dim_x}\"] = u @ s_new_mat @ v[coordinate_mask]\n",
    "\n",
    "    # Sample initial data and simulate initial observations\n",
    "    init_sample: Real[Array, \"{dim_x}\"] = mixt.sample(key_init_sample)\n",
    "\n",
    "    init_obs: Real[Array, \"{dim_y}\"] = a_recon @ init_sample\n",
    "    init_obs += random.normal(key_init_obs, init_obs.shape) * noise_std\n",
    "\n",
    "    # Construct observation noise covariance matrix\n",
    "    sigma_y = jnp.diag(jnp.full(dim_y, noise_std**2))\n",
    "\n",
    "    return a_recon, sigma_y, u, s_new, v, coordinate_mask, init_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actualy build/realise some samples from our prior model (i.e. $p(x_0)$, the equally spaced\n",
    "gride GMM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prior (equal weighted, grid GMM)\n",
    "means = [\n",
    "    jnp.array([-size * i, -size * j] * (dim_x // 2))\n",
    "    for i in range(center_range[0], center_range[1] + 1)\n",
    "    for j in range(center_range[0], center_range[1] + 1)\n",
    "]\n",
    "weights = jnp.ones(len(means))\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "ou_mixt_fun = Partial(ou_mixt, means=means, dim_x=dim_x, weights=weights)\n",
    "mixt = ou_mixt_fun(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either sample from the prior analytically directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get analytic prior samples\n",
    "analytic_prior_samples = mixt.sample(key, (1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot analytic prior samples\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Axes\n",
    "ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "ax.axvline(0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "# Samples\n",
    "ax.scatter(\n",
    "    x=analytic_prior_samples[:, 0],\n",
    "    y=analytic_prior_samples[:, 1],\n",
    "    color=COLOR_ALGORITHM,\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"black\",\n",
    "    lw=0.5,\n",
    "    s=10,\n",
    ")\n",
    "\n",
    "# Limits\n",
    "ax.set_xlim(*chart_lims)\n",
    "ax.set_ylim(*chart_lims)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"Coordinate 1\")\n",
    "ax.set_ylabel(\"Coordinate 2\")\n",
    "ax.set_title(\"Analytic Prior samples\")\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can sample using a diffusion model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model prior samples (i.e. DDPM)\n",
    "key, sub_key = random.split(key)\n",
    "\n",
    "sde = VP(jnp.array(beta_min), jnp.array(beta_max))\n",
    "model = get_model_fn(ou_mixt_fun, sde)  # epsilon estimator\n",
    "\n",
    "sampler = get_sampler(\n",
    "    SamplerName.DDIM_VP,\n",
    "    num_steps=num_steps,\n",
    "    shape=(num_samples, dim_x),\n",
    "    model=model,\n",
    "    beta_min=beta_min,\n",
    "    beta_max=beta_max,\n",
    "    eta=1.0,  # NOTE: equates to using DDPM\n",
    "    stack_samples=True,\n",
    ")\n",
    "\n",
    "# NOTE: lead axis of `prior_samples` is such that index 0 corresponds to X_0 (not X_T).\n",
    "prior_samples: Real[Array, \"{num_steps} {num_samples} {dim_x}\"] = sampler.sample(sub_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model prior samples\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Axes\n",
    "ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "ax.axvline(0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "# Samples\n",
    "ax.scatter(\n",
    "    x=prior_samples[0, :, 0],\n",
    "    y=prior_samples[0, :, 1],\n",
    "    color=COLOR_ALGORITHM,\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"black\",\n",
    "    lw=0.5,\n",
    "    s=10,\n",
    ")\n",
    "\n",
    "# Limits\n",
    "ax.set_xlim(*chart_lims)\n",
    "ax.set_ylim(*chart_lims)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"Coordinate 1\")\n",
    "ax.set_ylabel(\"Coordinate 2\")\n",
    "ax.set_title(\"Prior samples\")\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffusion model prior is generative, starting from $\\mathcal{N}(\\mathbf{0}_{d_x}, I_{d_x})$\n",
    "noise ($X_T$), and evolving according to a discretization of the backwards SDE of some forward\n",
    "process (determined by $\\beta_{min}$ and $\\beta_{max}$) using the score; $X_T \\rightarrow X_0$.\n",
    "We can create an animation showing this \"denoising\" process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation of prior sampling (i.e. uncondtional diffusion)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "skip = 10\n",
    "rev_subset_prior_samples = np.concatenate(\n",
    "    (prior_samples[::-skip], prior_samples[0][None, ...]), axis=0\n",
    ")\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    # Clear axis for next frame\n",
    "    ax.clear()\n",
    "\n",
    "    # Axes\n",
    "    ax.axhline(0, color=\"black\", lw=0.5)\n",
    "    ax.axvline(0, color=\"black\", lw=0.5)\n",
    "\n",
    "    # Samples\n",
    "    ax.scatter(\n",
    "        x=rev_subset_prior_samples[i, :, 0],\n",
    "        y=rev_subset_prior_samples[i, :, 1],\n",
    "        color=COLOR_ALGORITHM,\n",
    "        alpha=0.5,\n",
    "        edgecolors=\"black\",\n",
    "        lw=0.5,\n",
    "        s=10,\n",
    "    )\n",
    "\n",
    "    # Limits\n",
    "    ax.set_xlim(*chart_lims)\n",
    "    ax.set_ylim(*chart_lims)\n",
    "\n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Coordinate 1\")\n",
    "    ax.set_ylabel(\"Coordinate 2\")\n",
    "    ax.set_title(f\"Prior sample generation\\nt={num_steps - (i * skip)}\")\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(rev_subset_prior_samples), interval=100)\n",
    "plt.close()\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we realise the some observation matrix and an observation to setup the inverse problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inverse problem\n",
    "key, sub_key = random.split(key)\n",
    "\n",
    "(a, sigma_y, u, diag, v, coordinate_mask, init_obs) = generate_measurement_equations(\n",
    "    dim_x, dim_y, mixt, measurement_noise_std, sub_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have some observation $y$, which was collected according to:\n",
    "$$\n",
    "y := Ax_0^* + \\sigma_y\\epsilon,\\quad \\epsilon \\sim \\mathcal{N}\\left(\\mathbf{0}_{d_y}, I_{d_y}\\right)\n",
    "$$\n",
    "(where $x_0^*$ is some sample drawn from the analytic prior; the overall data distribution). Our\n",
    "goal then is to sample from the posterior $p(x_0 \\mid y)$.\n",
    "\n",
    "Again, we can exactly sample from the posterior in the case of this model; this is useful for\n",
    "comparison with our particle method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get posterior samples\n",
    "posterior = get_posterior(init_obs, mixt, a, sigma_y)\n",
    "key, sub_key = random.split(key)\n",
    "\n",
    "posterior_samples = posterior.sample(sub_key, (num_samples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true posterior samples\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Axes\n",
    "ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "ax.axvline(0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "# Samples\n",
    "ax.scatter(\n",
    "    x=posterior_samples[:, 0],\n",
    "    y=posterior_samples[:, 1],\n",
    "    color=COLOR_POSTERIOR,\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"black\",\n",
    "    lw=0.5,\n",
    "    s=10,\n",
    ")\n",
    "\n",
    "# Limits\n",
    "ax.set_xlim(*chart_lims)\n",
    "ax.set_ylim(*chart_lims)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"Coordinate 1\")\n",
    "ax.set_ylabel(\"Coordinate 2\")\n",
    "ax.set_title(\"Posterior samples\")\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we consider using SMC to target the posterior. We start by shrinking the measurement to 0 so\n",
    "that it aligns with the start $x_T^{(i)} \\sim \\mathcal{N}(0_{d_x}, I_{d_x})$ (and hence is close to\n",
    "$Ax_T^{(i)} \\approx 0_{d_y}$) for each particle $i$. The aim is to \"schedule\" this shrinkage\n",
    "that for every $t \\in \\{T, ..., 0\\}$, we have this proximity. There's many ways of shrinking; we can\n",
    "do so geometrically or according to the $\\alpha$ values associated with the prior model SDE (we\n",
    "use $\\sqrt{\\overline{\\alpha}_t}$ which aligns with the `MCGDiff` paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create auxiliary \"shrunk\" Y sequence\n",
    "\n",
    "# Geometrically shrunk\n",
    "alpha = 0.995\n",
    "alphas_geo = alpha ** np.arange(num_steps)[:, None]\n",
    "\n",
    "# SDE matching schedule\n",
    "alphas_sde = sampler.sqrt_alphas_cumprod[:, None]\n",
    "\n",
    "ys_geo = init_obs * alphas_geo\n",
    "ys_sde = init_obs * alphas_sde\n",
    "\n",
    "# NOTE: for particle filter since needs to run \"backwards\" (from T -> 0)\n",
    "ys_geo_rev = ys_geo[::-1]\n",
    "ys_sde_rev = ys_sde[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alpha schedule\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.axhline(0, color=\"black\", lw=0.5)\n",
    "\n",
    "for i in range(ys_geo.shape[1]):\n",
    "    plt.plot(ys_geo[:, i], label=f\"Coord {i} Geometric\")\n",
    "    plt.plot(ys_sde[:, i], label=f\"Coord {i} SDE\")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.xlabel(\"$t$\")\n",
    "plt.ylabel(\"$\\\\alpha_tY$\")\n",
    "plt.legend()\n",
    "plt.title(\"$\\\\alpha$ schedule comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric approach ends up being very diffult to tune properly, since for many values of $t$\n",
    "it isn't providing sufficiently information for guiding the particle filter (via the\n",
    "weight-resampling). Empirically, the one based on the SDE works very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our state space model:\n",
    "\\begin{align*}\n",
    "\\overline{X}_0 &\\sim \\mathcal{N}(0_{d_x}, I_{d_x}) \\\\\n",
    "\\overline{X}_t &\\sim p_{t \\mid t+1}(\\cdot \\mid X_{t+1}) \\\\\n",
    "Y_t &\\sim \\mathcal{N}(HX_t, \\sigma I_{d_y})\n",
    "\\end{align*}\n",
    "(where $p_{t \\mid t+1}(\\cdot \\mid X_{t+1})$ is our prior model (i.e. the unconditional DDPM))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgmm_ssm = SimpleObservationSSM(sampler=sampler, dim_x=dim_x, a=a, sigma_y=sigma_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our FK model uses the $p_{t \\mid t+1}$ transition of the SSM as the proposal but weights according\n",
    "to the ratio of likelihoods:\n",
    "\\begin{align*}\n",
    "\\omega_t^{(i)} &= \\frac{g(y_t \\mid \\overline{x}_t^{(i)})}{g(y_{t+1} \\mid x_{t+1}^{i})} \\\\\n",
    "&= \\frac{g(\\overline{\\alpha}_t^{\\frac{1}{2}} y \\mid \\overline{x}_t^{(i)})}{g(\\overline{\\alpha}_{t+1}^{\\frac{1}{2}} y \\mid x_{t+1}^{i})} \\\\\n",
    "&= \\frac{\\mathcal{N}\\left(\\overline{\\alpha}_t^{\\frac{1}{2}} y;\\ H\\overline{x}_t^{(i)}, \\sigma_y I_{d_y}\\right)}{\\mathcal{N}\\left(\\overline{\\alpha}_{t+1}^{\\frac{1}{2}} y;\\ Hx_{t+1}^{(i)}, \\sigma_y I_{d_y}\\right)}\n",
    "\\end{align*}\n",
    "(this is very similar a procedure to `MCGdiff` but with simplified covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_guided = LikelihoodGuidedPF(mgmm_ssm, data=ys_sde_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the particle filter, using systematic resampling and resampling when the ESS ratio drops\n",
    "below 90% (encourages more resampling which empirically worked a bit better). As a note here, this\n",
    "variable in conjuction with the scheduling provides a lot of flexibilty and it's unclear immediately\n",
    "what's optimal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_sampler = get_sampler(\n",
    "    name=SamplerName.SMC, fk_model=fk_guided, num_particles=num_samples, essr_min=0.9\n",
    ")\n",
    "\n",
    "particle_samples = particle_sampler.sample(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our final posterior samples compared with the true ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.axhline(0, color=\"black\", lw=0.5)\n",
    "ax.axvline(0, color=\"black\", lw=0.5)\n",
    "\n",
    "# True posterior samples\n",
    "ax.scatter(\n",
    "    x=posterior_samples[:, 0],\n",
    "    y=posterior_samples[:, 1],\n",
    "    color=COLOR_POSTERIOR,\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"black\",\n",
    "    lw=0.5,\n",
    "    s=10,\n",
    ")\n",
    "\n",
    "# Particle samples\n",
    "ax.scatter(\n",
    "    x=particle_samples[:, 0],\n",
    "    y=particle_samples[:, 1],\n",
    "    color=COLOR_ALGORITHM,\n",
    "    alpha=0.5,\n",
    "    edgecolors=\"black\",\n",
    "    lw=0.5,\n",
    "    s=10,\n",
    ")\n",
    "\n",
    "# Limits\n",
    "ax.set_xlim(*chart_lims)\n",
    "ax.set_ylim(*chart_lims)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel(\"Coordinate 1\")\n",
    "ax.set_ylabel(\"Coordinate 2\")\n",
    "ax.set_title(\"Particle posterior samples\")\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a nice animation showing the evolution of the guided particle filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation of particle posterior sampling\n",
    "subhist = [*particle_sampler.particle_history[::10], particle_sampler.particle_history[-1]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "\n",
    "def animate(i: int):\n",
    "    # Clear axis for next frame\n",
    "    ax.clear()\n",
    "\n",
    "    # Axes\n",
    "    ax.axhline(0, color=\"black\", lw=0.5)\n",
    "    ax.axvline(0, color=\"black\", lw=0.5)\n",
    "\n",
    "    # True posterior samples\n",
    "    ax.scatter(\n",
    "        x=posterior_samples[:, 0],\n",
    "        y=posterior_samples[:, 1],\n",
    "        color=COLOR_POSTERIOR,\n",
    "        alpha=0.5,\n",
    "        edgecolors=\"black\",\n",
    "        lw=0.5,\n",
    "        s=10,\n",
    "    )\n",
    "\n",
    "    # Particle samples\n",
    "    ax.scatter(\n",
    "        x=subhist[i][:, 0],\n",
    "        y=subhist[i][:, 1],\n",
    "        color=COLOR_ALGORITHM,\n",
    "        alpha=0.5,\n",
    "        edgecolors=\"black\",\n",
    "        lw=0.5,\n",
    "        s=10,\n",
    "    )\n",
    "\n",
    "    # Limits\n",
    "    ax.set_xlim(*chart_lims)\n",
    "    ax.set_ylim(*chart_lims)\n",
    "\n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Coordinate 1\")\n",
    "    ax.set_ylabel(\"Coordinate 2\")\n",
    "    ax.set_title(f\"Particle posterior sampling\\nt={num_steps - (i * 10)}\")\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(subhist), interval=100)\n",
    "plt.close()\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at histograms (for each coordinate) of the observation matrix applied to the\n",
    "true and particle posterior samples (true observation shown by black line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=dim_y, ncols=2, figsize=(8, 3 * dim_y))\n",
    "\n",
    "part_meas = particle_samples @ a.T\n",
    "true_meas = posterior_samples @ a.T\n",
    "\n",
    "for i in range(dim_y):\n",
    "    # True posterior samples re-measured\"\n",
    "    axs[i, 0].set_title(\"True posterior observations\")\n",
    "    axs[i, 0].set_ylabel(f\"Coordinate {i+1}\")\n",
    "    axs[i, 0].hist(true_meas[:, i], bins=20, color=COLOR_POSTERIOR)\n",
    "    axs[i, 0].axvline(init_obs[i], color=\"black\")\n",
    "\n",
    "    # Particle posterior samples re-measured\n",
    "    axs[i, 1].set_title(\"Particle posterior observations\")\n",
    "    axs[i, 1].hist(part_meas[:, i], bins=20, color=COLOR_ALGORITHM)\n",
    "    axs[i, 1].axvline(init_obs[i], color=\"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
