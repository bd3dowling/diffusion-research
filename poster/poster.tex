% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% Packages
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=custom,width=120,height=72,scale=0.94]{beamerposter}
\usepackage{graphicx}
\usepackage{anyfontsize}
\usepackage{subfig, stackengine}
\usepackage[style=mla]{biblatex}

\usetheme{gemini}
\usecolortheme{imperial}
\addbibresource{poster.bib}
\AtBeginBibliography{\footnotesize}

% Lengths
% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% Title
\title{Diffusion Models for Inverse Problems}

% Footer
\footercontent{
    \href{https://github.com/bd3dowling}{GH/@bd3dowling} \hfill
    MSc Statistic Poster Presentation \hfill
    \href{mailto:bid@ic.ac.uk}{bid23@ic.ac.uk}
}

% Logo
\logoleft{\vspace{-0.5cm}\hspace{1.15cm}\includegraphics[height=2cm]{poster/assets/imperial_logo.pdf}}
\logoright{\vspace{0.25cm}\stackanchor[8pt]{\Large Brendan Dowling}{\large Supervisor: Deniz Akyildiz}\hspace{1.25cm}}


% Body
\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

    \begin{block}{Background}
        \textbf{Linear inverse problems}: Given  a datapoint $\mathbf{x} \in \mathbb{R}^{d_x}$,
        denote some lossy measurement by
        $$
        \mathbf{y} = A\mathbf{x} + \sigma_y\epsilon \in \mathbb{R}^{d_y},\quad \epsilon \sim \mathcal{N}\left(0, \mathrm{I}_{d_y}\right)
        $$
        where
        $A \in \mathbb{R}^{d_y \times d_x}$ is some \emph{measurement matrix},
        and $\sigma_y \in \mathbb{R}^+$ controls the measurement noise. The goal of a linear
        inverse problem is to recover $\mathbf{x}$ from $\mathbf{y}$.
        Typically $d_x > d_y$, leading to a many-to-one $\mathbf{x} \rightarrow \mathbf{y}$ mapping
        (\cite{chungDiffusionPosteriorSampling2022}), and requiring some \emph{prior} information about $\mathbf{x}$.
        If we assume some prior distribution of the data, $p(\mathbf{x})$, ``solving'' the inverse problem
        amounts to sampling from some posterior:
        $$
        p(\mathbf{x} \mid \mathbf{y}) \propto p(\mathbf{x})g(\mathbf{y} \mid \mathbf{x}) = p(\mathbf{x})\mathcal{N}\left(\mathbf{y} \mid A\mathbf{x}, \sigma^2_y\mathrm{I}_{d_y}\right)
        $$
        For many problems, $p(\mathbf{x})$ is generally unknown or does not conjugate with $g(\mathbf{y} \mid \mathbf{x})$,
        necessitating numerical methods to sample from $p(\mathbf{x} \mid \mathbf{y})$.

        \begin{figure}[htbp]
            \centering
            \subfloat[Super-resolution]{%
                \includegraphics[width=0.48\colwidth]{poster/assets/super_resolution.png}
                \label{fig:sub1}
            }
            \hfill
            \subfloat[Inpainting]{%
                \includegraphics[width=0.48\colwidth]{poster/assets/inpainting.png}
                \label{fig:sub2}
            }
            \vspace{0.2cm}
            \subfloat[Gaussian deblur]{%
                \includegraphics[width=0.48\colwidth]{poster/assets/gaussian_deblur.png}
                \label{fig:sub3}
            }
            \hfill
            \subfloat[Motion deblur]{%
                \includegraphics[width=0.48\colwidth]{poster/assets/motion_deblur.png}
                \label{fig:sub4}
            }
            \caption{Examples of linear inverse problems for images (\cite{chungDiffusionPosteriorSampling2022})}
            \label{fig:main}
        \end{figure}
    
        \textbf{Diffusion models}: Diffusion models provide a means of sampling from intractable
        data distributions, $p(\mathbf{x})$, and have proven highly effective in high dimensional
        settings, such as image generation (\cite{dhariwalDiffusionModelsBeat2021}).
        They can be formulated by considering some forwards
        noising process and some backwards de-noising process (\cite{douDiffusionPosteriorSampling2023}).
        In their discrete representation, the forward process can be described by a Markov chain:
        \begin{equation}
        q(\mathbf{x}_{1:T} \mid \mathbf{x}_0) = \prod_{t=1}^T \mathcal{N}\left(a_t \mathbf{x}_{t-1}, b_t^2\mathrm{I}_{d_x}\right) \label{eq:fwd}
        \end{equation}
        where $\{(a_t, b_t)\}_{t=1}^T$ differ depending on formulation of the problem (\cite{songScoreBasedGenerativeModeling2021}).
        From this, we yield the marginal $q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(c_t\mathbf{x}_0, d_t^2\mathrm{I}_{d_x})$
        where $c_t, d_t$ are derived from $a_t$ and $b_t$.
        We assume the backwards process to similarly be a Markov chain
        and train an $\epsilon$ (\cite{hoDenoisingDiffusionProbabilistic2020}) or score (\cite{songScoreBasedGenerativeModeling2021}) approximating
        neural network, $s_\theta(\mathbf{x}_t, t)$, to
        estimate the score function $\nabla_{\mathbf{x_t}}\log q(\mathbf{x}_t \mid \mathbf{x}_0)$ so
        to enable sampling from:
        $$
        p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}\left(u_t\cdot \frac{\mathbf{x}_t + d_t^2 s_\theta(\mathbf{x}_t, t)}{c_t} + v_ts_\theta(\mathbf{x}_t, t), w_t^2\mathrm{I}_{d_x}\right)
        $$
        with $u_t, v_t$ some functions of $a_t, b_t$, and the mean derived from Tweedie's formula (\cite{douDiffusionPosteriorSampling2023}).

        \begin{figure}[htbp]
            \centering
            \includegraphics[width=0.95\colwidth]{poster/assets/ddpm_flow.png}
            \caption{DDPM graphical model (\cite{hoDenoisingDiffusionProbabilistic2020})}
            \label{fig:ddpm_flow}
        \end{figure}
    \end{block}
\end{column}

\separatorcolumn

\begin{column}{\colwidth}

    \begin{block}{Problem Overview}
        Using diffusion models for solving inverse problems amounts to guiding each step of
        the backwards process towards a region on the data manifold which is sensibly explains
        the measurement. This is achieved by considering $p(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{y})$
        transitions, and replacing the unconditional score with the conditional one (\cite{chungDiffusionPosteriorSampling2022}):
        \begin{equation}
        \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t \mid \mathbf{y}) = \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) + \nabla_{\mathbf{x}_t}\log p_t(\mathbf{y} \mid \mathbf{x}_t) \label{eq:cond-score}
        \end{equation}
        The first term in Equation \ref{eq:cond-score} is exactly what the unconditional score network,
        $s_\theta(\mathbf{x}_t, t)$, estimates. The second term, however, is generally difficult
        to compute due to the time $t$ dependence (\cite{chungDiffusionPosteriorSampling2022});
        many different methods (\cite{chungDiffusionPosteriorSampling2022}, \cite{song2023pseudoinverseguided}, \cite{boysTweedieMomentProjected2023}) aim to approximate it.

        Significant recent research (\cite{cardosoMonteCarloGuided2023}, \cite{trippeDiffusionProbabilisticModeling2023}, \cite{wuPracticalAsymptoticallyExact2023}, \cite{janatiDivideandConquerPosteriorSampling2024})
        has considered using SMC methods to instead more directly target the posterior and circumvent
        the need to compute this term. Below, we describe a very simple SMC algorithm which aligns closely
        with the MCGdiff algorithm of \cite{cardosoMonteCarloGuided2023} except with a much simpler
        covariance structure.

        We note here that the focus is on taking pre-trained, unconditional score models and using
        them for general linear inverse problems. One could train a model specific to
        each inverse problem, with such a model then providing conditional scores out of the box.
        However, such a model will not be usable on other inverse tasks.
        Furthermore, training such a conditional model requires paired/labeled data
        which may not be easily available; this also has implications for generalization/few-shot
        learning. That said, when score is limited and compute (for training) is not an issue,
        such conditional models provide state-of-the-art performance.

    \end{block}

    \begin{alertblock}{A Simple SMC Approach}
        Consider the state space model (SSM):
        \begin{align*}
        \mathbf{X}_t &\sim p_{t \mid t+1}(\cdot \mid \mathbf{X}_{t+1}),\quad
        \mathbf{X}_T \sim \mathcal{N}(0_{d_x}, \mathrm{I}_{d_x}) \\
        \mathbf{Y}_t \mid \mathbf{X}_t &\sim \mathcal{N}(H\mathbf{X}_t, \sigma \mathrm{I}_{d_y})
        \end{align*}
        where $p_{t \mid t+1}(\cdot \mid \mathbf{X}_{t+1})$ is the diffusion prior model.
        The diffusion model begins at $\mathbf{X}_T$, near $0_{d_x}$, and iteratively moves
        towards some $\mathbf{x}_0$ on the data manifold.
        However, a measurement, $\mathbf{y}$, is only available at $t=0$,
        and is likely to live far from $\mathbf{y}_t$ (according to the SSM) for large $t$.
        Hence, we construct an auxiliary sequence $\{\mathbf{y}_t\}_{t=1}^T$ according to
        \begin{align*}
        \mathbf{y}_t &= \rho_t \mathbf{y}_{t-1},\quad y_0 = y
        \end{align*}
        with $0 < \rho_{t-1} < \rho_t < 1$, so to align the initial and intermediate measurements with the prior model.
        Loosely, this enables ``smoother'' guidance of diffusion process towards $p(\mathbf{x}_0 \mid \mathbf{y})$
        samples. A sensible choice for $\{\rho_t\}_{t=1}^T$ are the coefficients $\{a_t\}_{t=1}^T$ that define
        the forward transition in Equation \ref{eq:fwd}.

        Define a Feynman-Kac model (\cite{chopinIntroductionSequentialMonte2020}) with the
        $p_{t \mid t+1}$ transition of the SSM as the proposal but with weights for each $i$ particle
        set according to the ratio of likelihoods:
        $$
        \omega_t^{(i)} = \frac{g\left(\mathbf{y}_t \mid \tilde{\mathbf{x}}_t^{(i)}\right)}{g\left(\mathbf{y}_{t+1} \mid \mathbf{x}_{t+1}^{i}\right)}
        = \frac{g\left(\overline{\alpha}_t^{\frac{1}{2}} \mathbf{y} \mid \tilde{\mathbf{x}}_t^{(i)}\right)}{g\left(\overline{\alpha}_{t+1}^{\frac{1}{2}} \mathbf{y} \mid \mathbf{x}_{t+1}^{i}\right)}
        = \frac{\mathcal{N}\left(\overline{\alpha}_t^{\frac{1}{2}} \mathbf{y};\ H\tilde{\mathbf{x}}_t^{(i)}, \sigma_y I_{d_y}\right)}{\mathcal{N}\left(\overline{\alpha}_{t+1}^{\frac{1}{2}} \mathbf{y};\ H\mathbf{x}_{t+1}^{(i)}, \sigma_y I_{d_y}\right)}
        $$

        Running a particle filter will then provide approximate posterior samples. Note here that we can easily use
        a DDIM (\cite{songDenoisingDiffusionImplicit2020}) sampler to reduce the number of timesteps/iterations.
    \end{alertblock}

    \begin{block}{Future Work}
        \begin{itemize}
            \item Consider more complex covariance structures, such as those of \cite{cardosoMonteCarloGuided2023}.
            \item Consider more complex likelihood models and observation noise scheduling.
            \item Examine performance (quality and efficiency) on image-based task.
            \item Consider link with twisting functions (\cite{wuPracticalAsymptoticallyExact2023}).
        \end{itemize}
    \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

    \begin{exampleblock}{Example}
        \heading{Setup}
    
        Consider the mixture model, $q_{\text{data}}$, example of \cite{cardosoMonteCarloGuided2023},
        with 25 equally weighted $d_x$-dimensional Gaussian random variables with means
        $\mathbf{\mu}_{i,j} := (8i, 8j, \dots, 8i, 8j) \in \mathbb{R}^{d_x}$
        for $(i, j)\in \{-2, \ldots, 2\}^2$ and unit variance.
        We generate some $d_y$-dimensional measurement, $\mathbf{y}$, according to the following process:
        \vspace{-1cm}
        \begin{itemize}
            \item Draw $\tilde{A} \sim \mathcal{N}(0, 1)^{d_y \times d_x} \in \mathbb{R}^{d_y \times d_x}$
            and compute its SVD decomposition, $USV^\top$.
            \item Sample $s_{i,j} \sim \mathcal{U}[0,1]$ for $(i,j) \in \{-2, \dots, 2\}^2$.
            \item Set $A := U\, diag(\{s_{i,j}\})\, V^\top$.
            \item Draw $\mathbf{x}_* \sim q_{\text{data}}$ and set $\mathbf{y} := A\mathbf{x}_* + \sigma_y\epsilon,\ \epsilon \sim \mathcal{N}(\mathbf{0}_{d_y}, \mathrm{I}_{d_y})$
        \end{itemize}
        For this model, the posterior, $p(\mathbf{x}_* \mid \mathbf{y})$, is available in closed form
        (another GMM; see \cite{cardosoMonteCarloGuided2023}) which enables us to benchmark
        numerical methods. The backwards marginal is also available in closed form allowing
        us to avoid training a score network (but still using DDPM sampling).
    
        \heading{Results}
    
        We run our simple SMC algorithm for this model, using 1000 particles and 
        adaptive systematic resampling when the ESS dropped below 90\%.
        We considered many different values for $d_x$ and $d_y$; Figure \ref{fig:example}
        corresponds to $d_x = d_y = 2$.
        From \ref{fig:example}, we see it has well targeted the true posterior distribution.
        Empirically (via sliced Wasserstein distance) the algorithm retained performance in
        higher dimensional settings, in spite of its simplified covariance
        structure, which provides some promise for applications in image-based settings.

        \begin{figure}[htbp]
            \centering
            \includegraphics[width=0.95\colwidth]{poster/assets/samples.png}
            \caption{SMC performance on GMM example}
            \label{fig:example}
        \end{figure}
    \end{exampleblock}

    \begin{block}{References}
        \printbibliography
    \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
