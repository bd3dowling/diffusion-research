\chapter{Introduction}

Score-based diffusion models
\parencite{pmlr-v37-sohl-dickstein15,hoDenoisingDiffusionProbabilistic2020,songGenerativeModelingEstimating2020}
have garnered significant attention for their ability to model complex data distributions,
particularly in high-dimensional spaces such as image
\parencite{dhariwalDiffusionModelsBeat2021,rombachHighResolutionImageSynthesis2021},
video \parencite{hoVideoDiffusionModels2022}, and molecular
\parencite{xuGeoDiffGeometricDiffusion2022} synthesis. These models operate by iteratively
transforming noise into structured data, using a learned (Stein) score function, to effectively enable
sampling from the underlying data distribution. Consequently, diffusion models have recently been
considered as prior models for Bayesian posterior sampling tasks such as solving inverse problems
\parencite{songGenerativeModelingEstimating2020} and, more recently, optimisation
\parencite{krishnamoorthyDiffusionModelsBlackBox2023,guoGradientGuidanceDiffusion2024,kongDiffusionModelsConstrained2024}.

There are roughly two core approaches to this posterior sampling task: training a conditional score
network for the specific inverse problem
\parencite{songGenerativeModelingEstimating2020,nicholGLIDEPhotorealisticImage2021, sahariaPaletteImagetoImageDiffusion2021,sahariaPhotorealisticTexttoImageDiffusion2022}
or optimisation task \parencite{krishnamoorthyDiffusionModelsBlackBox2023,guoGradientGuidanceDiffusion2024};
taking a pre-trained score network and guiding the generative process using approximations of the
conditional score for inverse problems
\parencite{chungDiffusionPosteriorSampling2022,song2023pseudoinverseguided,boysTweedieMomentProjected2023}
or gradients of the objective for optimisation \parencite{kongDiffusionModelsConstrained2024}. We
focus on the latter approach since it enables zero shot inference --- we can take any diffusion
model and use it for any inverse/optimisation tasks without needing access to labelled samples.

We can view inverse problems as a special case of optimisation where the objective is the posterior
density (i.e. maximum \emph{a posteriori} estimation of the state (data) from the
observations/conditioning variable). Hence, through the optimisation lens, we may view the prior
diffusion model as imposing constraints on the optimisation procedure. These constraints ensure
that outcomes of the optimisation procedure lie on or near the data manifold, representing
realistic configurations \parencite{kongDiffusionModelsConstrained2024}.

Recent work has considered using sequential Monte Carlo (SMC) methods
\parencite{chopinIntroductionSequentialMonte2020} to correct for the approximation errors incurred
during the guiding process
\parencite{trippeDiffusionProbabilisticModeling2023,cardosoMonteCarloGuided2023,douDiffusionPosteriorSampling2023,wuPracticalAsymptoticallyExact2023}.
However, such frameworks have strictly focused on solving linear inverse problems.
In this paper, we introduce \texttt{SMCDiffOpt}, a versatile framework that combines diffusion models
with SMC methods to address both general optimization and inverse problems. Our framework views
optimization as a posterior sampling problem from an annealed Boltzmann distribution, using
diffusion models as priors and employing SMC methods to more effectively target the posterior
distribution. It's also flexible, operating in zero-shot, accommodating different proposal
distributions, and being able to operate without access to gradients of the objective --- this makes
it particularly effective in black-box optimization settings where only function evaluations are
possible.

We contrast \texttt{SMCDiffOpt} with standard gradient-based optimizers on a synthetic optimisation
task, highlighting its advantage at navigating complex optimization landscapes, including those
with multiple optima. We also test it on the real-world SuperConductor black-box optimisation task
\parencite{trabuccoDesignBenchBenchmarksDataDriven2022}, benchmarking it against other
state-of-the-art approaches. Additionally, we demonstrate its performance and flexibility in solving
inverse problems by comparing its posterior sampling efficiency to alternative methods on a
synthetic Gaussian mixture model example.

The paper is organized as follows: \hyperref[chap:background]{Section 2} reviews the relevant
background on optimization, inverse problems, diffusion models, and SMC methods, as well as
mentioning some related and alternative approaches. \hyperref[chap:methods]{Section 3} introduces
the \texttt{SMCDiffOpt} framework, detailing its application to both general optimization tasks and
inverse problems.  \hyperref[chap:experiments]{Section 4} presents experimental results, and
\hyperref[chap:discussion]{Section 5} discusses the broader implications of our findings and
suggests directions for future research.
