\chapter{SMC-Guided Conditional Diffusion Sampling} \label{chap:methods}

\section{\texttt{SMCDiffOpt} for General Optimisation Problems} \label{sec:gen-opt}

Consider some function $f: \mathcal{X} \to \mathbb{R}$ whose minima, $\{\mathbf{x}^*_i\}_{i=1}^M$,
we wish to find. Consider $q(\mathbf{x}; \gamma) \propto \exp\{-\gamma f(\mathbf{x})\}$. Suppose we
have some pre-trained score network which we can use to sample from $p_{\text{data}}$ via some
$p_\theta$ in $T$ time-steps. Take $p_\theta$ as a prior for use in  \autoref{prop:opt-sampling}. Write
$\mathbf{x}_0 := \mathbf{x}$. Naive targeting of
$\pi(\mathbf{x}_0) \propto q(\mathbf{x}_0; \beta)p_\theta(\mathbf{x}_0)$ amounts to sampling
$\mathbf{x}_0$ according to \autoref{eq:uncond-sampling}, and using some form of MCMC-type sampling with
annealing for $\beta$ to target \autoref{eq:opt-target-inf}. In cases where $f$ is non-convex and/or
induces a highly rugged loss landscape, sampling can be highly inefficient or challenging
to tune \parencite{kongDiffusionModelsConstrained2024}.

Instead, we consider using the intermediate unconditional transition densities of the diffusion
model, $p_\theta(\mathbf{x}_{t} \mid \mathbf{x}_{t+1})$, and guide these with the objective
function. Using some annealing schedule, $\gamma(t)$, we construct a sequence
$\{q_t(\mathbf{x}_t; \gamma_t)\}_{t=0}^T$. Loosely (though concretely in
\autoref{sec:inv-prob-spec-case}), we may view $q_t$ as a likelihood, with $f$ emitting observations
$\mathbf{y}_t$ based on $\mathbf{x}_t$. That is, we may consider $q_t(\mathbf{x_t}; \gamma_t)$
as some $g(\mathbf{y}_t \mid \mathbf{x}_t)$. The task is now frameable through a Bayesian filtering
lens, with the intermediate targets being
\begin{equation}
    p(\mathbf{x}_t \mid \mathbf{y}_{T:t}) \propto p(\mathbf{x}_t \mid \mathbf{y}_{T:t+1})g(\mathbf{y}_t \mid \mathbf{x}_t)
\end{equation}
(as in \autoref{eq:bayesian-filtering-2} with time reversed). SMC methods provide a principled
method of targeting these and ultimately
$p(\mathbf{x}_0 \mid \mathbf{y}_{0:T}) \approx q_\infty \propto q(\mathbf{x}; \infty)p_\theta(\mathbf{x})$
of \autoref{prop:opt-sampling}, yielding $\{\mathbf{x}^*_i\}_{i=1}^M$.

\begin{remark}[Diffusion Models Learn Data Manifolds]
    A key feature which enables $p_\theta$ to act as a good prior is the ability of diffusion models
    to implicitly learn the data manifold \parencite{debortoliDiffusionSchrOdinger2021,pidstrigachScoreBasedGenerativeModels2022,wenliangScorebasedGenerativeModels2023}.
    In the context of optimisation, the diffusion model essentially enables us to only consider
    optimisation over `sensible' values of $\mathbf{x}$; a feasibility constraint. Put differently,
    the unconditional diffusion prior acts as a regularisation to encourage optimisation of $f$ over
    $\mathcal{X} \subseteq \mathbb{R}^{d_x}$ as opposed to over all of $\mathbb{R}^{d_x}$
    \parencite{guoGradientGuidanceDiffusion2024}.
\end{remark}

If we consider the \emph{bootstrap} proposal,
$r(\cdot \mid \mathbf{x}_{t+1}) = p_\theta(\cdot \mid \mathbf{x}_{t+1})$, then the
weight function in \autoref{eq:weight-func-gen} reduces to
$g(\mathbf{y}_t \mid \mathbf{x}_t) = q_t(\mathbf{x}; \gamma_t)$, the `likelihood'. In the case where
$q_t(\mathbf{x}_t; \gamma_t)$ is `peaky' over its support (which is guaranteed for $t$ close to 0
assuming reasonable annealing), this will generally be a poor proposal
\parencite{chopinIntroductionSequentialMonte2020}, as it corresponds to the `observations',
$\mathbf{y}_t$, being informative about the `states', $\mathbf{x}_t$, and us ignoring those
observations. The obvious remedy is to choose a better proposal. In the case of general
optimisation, however, this is a non-trivial task; recall the `observations' here are not available
\emph{a priori}. Incorporation of the gradient of $f$ might be one possibility, but this requires
differentiability of $f$.

Instead, we consider using an \emph{auxiliary variable}, adjusting the weight
for each particle based on their \emph{previous} likelihood. Heuristically, this implies we weight
more heavily those particles whose likelihood \emph{significantly increased} compared to their
previous position. This imposes a form of regularisation that implicitly discourages concentration on
high likelihood regions near the initial distribution, $p(\mathbf{x}_T)$. Geometrically, this
regularisation may be viewed as dynamically flattening the optimisation landscape for each particle
based on its position. The role of $\gamma(t)$ is to provide global annealing of the landscape,
gradually attenuating this dynamic effect and ultimately applying \autoref{prop:opt-sampling} to
enable sampling from $Q_\infty$. The full algorithm is described in \autoref{alg:smc-opt}.

\begin{algorithm}[ht]
    \caption{\texttt{SMCDiffOpt} for General Optimisation} \label{alg:smc-opt}
    \begin{algorithmic}
        \Require $f: \mathcal{X} \to \mathbb{R}$, an objective function to \emph{minimise} (take $-f$ to \emph{maximise}).
        \Require $s_\theta(\mathbf{x}_t, t)$, a pre-trained score network (or noise-predictor and apply \autoref{prop:score-to-noise}).
        \Require $r(\cdot \mid \mathbf{x}_{t+1})$, a proposal distribution. Default to $p_\theta(\cdot \mid \mathbf{x}_{t+1})$.
        \Require $\gamma(t)$, some annealing schedule.
        \Require $N$, number of particles to use.
        \State Draw $\mathbf{x}_T^{(i)} \sim \mathcal{N}(0, \mathbf{I}_{d_x}),\quad i=1,\ldots,N$
        \For{$t \gets T-1$ to $0$}
            \For {$i \gets 1$ to $N$}
                \State \textbf{Move}: $\tilde{\mathbf{x}}_{t}^{(i)} \sim r(\cdot \mid \mathbf{x}_{t+1})$.
                \State \textbf{Compute weight}:
                \begin{equation}
                    \omega_t^{(i)} \gets \frac{\exp\left\{-\gamma(t) f(\tilde{\mathbf{x}}_t^{(i)})\right\}}{\exp\left\{-\gamma(t+1) f(\mathbf{x}_{t+1}^{(i)})\right\}}. \label{eq:weight-formula}
                \end{equation}
                \State \textbf{Normalise weight}: $W_t^{(i)} \gets \frac{\omega_t^{(i)}}{\sum_{i=1}^N \omega_t^{(i)}}$.
                \State \textbf{Resample}: $\{\mathbf{x}_{T:t}^{(i)}\}_{i=1}^N \sim \text{Multinomial}\left(\{\tilde{\mathbf{x}}_{T:t}^{(i)}\}_{i=1}^N; \{W_t^{(i)}\}_{i=1}^N\right)$.
            \EndFor
        \EndFor
        \State \textbf{return} $\{\mathbf{x}_0^{(i)}\}_{i=1}^N$, some summary statistic computed
        using $\{w_0^{(i)}\}_{i=1}^N$, or some $\{\mathbf{x}_0^{(i)}\}_{i \in \mathcal{I}}$
        subset which achieve the (perhaps within a threshold) minimal value amongst all particles.
    \end{algorithmic}
\end{algorithm}

\begin{remark}[Auxiliary Variable]
    The concept of using an auxiliary variable to adjust the weights is the foundation of the
    \emph{auxiliary particle filter} \parencite{chopinIntroductionSequentialMonte2020} and can be
    justified analytically. The exact procedure for auxiliary particle filters differs very slightly
    from that described in \autoref{def:smc}, mainly pertaining to the order of operations and how the
    intermediate distributions are approximated from the particles.
\end{remark}

\begin{remark}[Gradient-free] \label{rem:grad-free}
    Note that we don't require any gradient information about $f$. This makes \autoref{alg:smc-opt}
    suitable in black-box optimisation settings where we might only be able to evaluate $f$ and
    can't access its gradient (by back-propagation or analytically). However, where we can compute
    the gradient, $\nabla_{\mathbf{x}_{t}} f(\mathbf{x}_{t})$, we can instead consider using
    these gradients to `twist' \parencite{wuPracticalAsymptoticallyExact2023} the proposal
    distribution, yielding a proposal $r_t(\cdot \mid \mathbf{x}_{t+1})$ closer to the true
    `posterior', $p(\cdot \mid \mathbf{x}_{t+1}, \mathbf{y}_t)$, reducing the variance of the
    weights and making the sampler more efficient (enabling lower number of particles, $N$).
    We discuss alternative proposals in more detail in \autoref{sec:relation-to-related}, though
    these typically are only applicable in inverse problem settings.
    Alternatively, we may consider adding additional steps after moving the particles, nudging
    (perhaps a subset of) the particles as in \textcite{akyildizNudgingParticleFilter2020}.
    Note here though that even with access to gradients, the algorithm offers substantial
    advantages over standard gradient-based optimisation in cases where $f$ has steep gradients
    or is highly non-convex. We demonstrate this experimentally in \autoref{sec:branin}.
\end{remark}

\begin{remark}[Annealing-only]
    In principle we could remove the auxiliary variable, and just tune the $\gamma(t)$
    annealing schedule. In practice, however, this is extremely challenging and adds even heavier
    dependence on the nature of $f$; in cases where it has steep gradients, the annealing schedule
    needs to be very gentle, which is often infeasible due to the fixed $T$ time-steps that the
    diffusion sampler takes (one would need to consider intermediate diffusions with, for example, extra
    Langevin steps in a fashion like \textcite{janatiDivideandConquerPosteriorSampling2024} to work
    around). The auxiliary variable helps make the tuning of $\gamma(t)$ much easier by mitigating
    its importance in adequately flattening the landscape. This was observed strongly in
    \autoref{sec:branin}, for example.
\end{remark}

\begin{remark}[Adaptive Resampling \parencite{chopinIntroductionSequentialMonte2020}]
    In practice, we don't usually resample at every iteration in \autoref{alg:smc-opt}, instead
    resampling only when particle diversity falls below some certain threshold (typically using
    effective sample size as a metric). Given this, the weight formula in \autoref{eq:weight-formula} is
    adjusted by pre-multiplying by the previous un-normalised weight (or 1 if resampled); the
    procedure is otherwise identical. The choice of, say, ESS ratio is critical though for
    \autoref{alg:smc-opt} and can significantly alleviate the difficulty in tuning $\gamma(t)$.
\end{remark}

\section{Inverse Problems as a Special Case} \label{sec:inv-prob-spec-case}

\begin{proposition}[Obvservation Generation] \label{prop:obs-gen}
    Let $\mathbf{y} = \mathbf{y}_0 = A\mathbf{x}_0 + \sigma_y\epsilon$ be some noisy linear
    measurement we have about a desired sample, $\mathbf{x}_0$. Construct a sequence
    $\{\mathbf{y}_t\}_{t=1}^T$ by $\mathbf{y}_t = a_t\cdot \mathbf{y}_{t-1}$ with $a_t$ of
    \autoref{def:forward-process}. It follows that
    \begin{equation}
        \mathbf{y}_t = c_t\cdot \mathbf{y} \label{eq:obs-gen}
    \end{equation}
    from which we may conclude
    \begin{equation*}
        \mathbf{Y}_t \mid \mathbf{X}_0 = \mathbf{x}_0 \sim \mathcal{N}(c_t\cdot A\mathbf{x}_0, c_t^2\sigma_y^2\cdot \mathbf{I}_{d_y})
    \end{equation*}
    and
    \begin{equation}
        \mathbf{Y}_t \mid \mathbf{X}_t = \mathbf{x}_t \sim \mathcal{N}(A\mathbf{x}_t, c_t^2\sigma_y^2\cdot \mathbf{I}_{d_y} + d_t^2\cdot AA^\top) \label{eq:obs-likelihood}
    \end{equation}
    with $g(\mathbf{y}_0 \mid \mathbf{x}_0) = g(\mathbf{y} \mid \mathbf{x}_0) = \mathcal{N}(\mathbf{y}; A\mathbf{x}_0, \sigma_y^2\cdot \mathbf{I}_{d_y})$
    exactly the measurement density.
\end{proposition}

\begin{proof}
    See \hyperref[prf:obs-generation]{Appendix B}.
\end{proof}

\begin{remark}[Shrinking] \label{rem:shrinking}
    The important characteristic of \autoref{eq:obs-gen} is that it `shrinks' the
    observations towards 0 (in tandem, theoretically, with how $\mathbf{x}_0$
    is shrunk towards zero according to its forward process). This essentially creates an
    \emph{aligned} observation sequence so that the likelihoods (i.e. density evaluations of
    \autoref{eq:obs-likelihood}) are sensibly sized, enabling their usage in SMC algorithms.
\end{remark}

Note that this generation does not depend on the clean sample, $\mathbf{x}_0$. As such,
it can be generated prior to running the guided sampling and we may view sampling from
$p(\mathbf{x}_0 \mid \mathbf{y})$ as a Bayesian filtering task. This is described succinctly in
\textcite{douDiffusionPosteriorSampling2023}:
\begin{quote}
    We can generate a sample $\mathbf{x}_0$ from the Bayesian posterior distribution
    $p_\theta(\mathbf{x}_0 \mid \mathbf{y}_0)$ by first sampling from
    $p_\theta(\mathbf{y}_{1:T} \mid \mathbf{y}_0)$ before sampling from
    $p_\theta(\mathbf{x}_0 \mid \mathbf{y}_{0:T})$. This is because
    $p_\theta(\mathbf{x}_0 \mid \mathbf{y}_0) = \int p_\theta(\mathbf{x}_0 \mid \mathbf{y}_{0:T})p_\theta(\mathbf{y}_{1:T} \mid \mathbf{y}_0)\, d\mathbf{y}_{1:T}$.
\end{quote}

\begin{proposition}[Inverse Problem Objective]
    Consider the objective function
    \begin{equation}
        f(\mathbf{x}_t) = -\frac{1}{2\sigma_y^2c_t^2}\Vert\mathbf{y}_t - A\mathbf{x}_t\Vert^2 \label{eq:inverse-objective}
    \end{equation}
    with $\mathbf{y}_t$ constructed from $\mathbf{y}$ according to \autoref{eq:obs-gen}.
    Plugging this in to \autoref{eq:weight-formula}, ignoring particle indices, the weight function
    reduces to (by proportionality)
    \begin{equation}
        \omega_t \gets \frac{\mathcal{N}(\mathbf{y}_t; A\tilde{\mathbf{x}}_t, c_t^2\sigma_y^2\cdot \mathbf{I}_{d_y} + d_t^2\cdot AA^\top)^{\gamma(t)}}{\mathcal{N}(\mathbf{y}_{t+1}; A\mathbf{x}_{t+1}, c_{t+1}^2\sigma_y^2\cdot \mathbf{I}_{d_y} + d_{t+1}^2\cdot AA^\top)^{\gamma(t+1)}}. \label{eq:weight-formula-inverse}
    \end{equation}
\end{proposition}

We see this is exactly the ratio of the likelihoods, annealed according to $\gamma(t)$. Its
interpretation is exactly as in \autoref{sec:gen-opt} except the notion of observations and posterior
densities is now exact.

\begin{remark}[MAP]
    The choice of $f$ in \autoref{eq:inverse-objective} essentially states that
    we're choosing to maximise the likelihood, $g(\mathbf{y}_t \mid \mathbf{x}_t)$, subject to the
    constraint/regularisation imposed by the prior diffusion model through
    $p_\theta(\cdot \mid \mathbf{x}_{t+1})$. Based on \ref{def:smc}, it follows that running
    \autoref{alg:smc-opt} seeks ultimately to yield those samples $\{\mathbf{x}_0^{(i)}\}_{i=1}^N$
    which are the \emph{maximum a posteriori} estimates of $\mathbf{x}_0$, exactly as desired for
    solving an inverse problem.
\end{remark}

\begin{remark}[Inner Tempering]
    By symmetry of the Gaussian distribution, we have
    \begin{align*}
        \mathcal{N}(\mathbf{y}_t; A\mathbf{x}_t, c_t^2\sigma_y^2\cdot \mathbf{I}_{d_y} + d_t^2\cdot AA^\top) &= \mathcal{N}(A\mathbf{x}_t; \mathbf{y}_t, c_t^2\sigma_y^2\cdot \mathbf{I}_{d_y} + d_t^2\cdot AA^\top) \\
        &= \mathcal{N}(A\mathbf{x}_t; c_t\mathbf{y}, c_t^2\sigma_y^2\cdot \mathbf{I}_{d_y} + d_t^2\cdot AA^\top)
    \end{align*}
    We see that there is essentially an inner-level of tempering according to $c(t)$, defined by the
    unconditional diffusion model. As such, for inverse problems we can generally take
    $\gamma(t) = 1\, \forall t$.
\end{remark}

\section{Extensions and Related Work} \label{sec:relation-to-related}

Taking $\mu_{t+1}(\mathbf{x}_{t+1}, \theta), \Sigma_{t+1}$ as the mean (e.g. \autoref{eq:ddpm-mu}) and
variance (e.g. \autoref{eq:ddpm-sigma}) of the backwards transition kernel,
$p_\theta(\mathbf{x}_t \mid \mathbf{x}_{t+1})$, and $\Xi_t$ as the covariance of
\ref{eq:obs-likelihood} the Monte Carlo Guided Diffusion (MCGdiff)
\parencite{cardosoMonteCarloGuided2023} and Filter Posterior Sampling SMC (FPS-SMC)
\parencite{douDiffusionPosteriorSampling2023} frameworks consider essentially equivalent proposals
\begin{align*}
    r(\tilde{\mathbf{x}}_{t} \mid \mathbf{x}_{t+1}, \mathbf{y}_t) &\propto p_\theta(\tilde{\mathbf{x}}_t \mid \mathbf{x}_{t+1})g(\mathbf{y}_{t} \mid \tilde{\mathbf{x}}_{t})
    = \mathcal{N}(\mu^{*}_{t+1}(\mathbf{x}_{t+1}, \mathbf{y}_t, \theta), \Sigma_{t+1}^{*})
\end{align*}
(by Normal-Normal conjugacy) with
\begin{align*}
    \Sigma_{t+1}^{*} &= \left(\Sigma_{t+1}^{-1} + A^\top \Xi^{-1} A \right)^{-1} \\
    \mu_{t+1}^{*}(\mathbf{x}_{t+1}, \mathbf{y}_t, \theta) &= \Sigma_{t+1}^{*}\cdot\left(\Sigma_{t+1}^{-1}\mu_{t+1}(\mathbf{x}_{t+1}, \theta) + A^\top \Xi^{-1} \mathbf{y}_t \right)
\end{align*}
and $\mathbf{y}_t$ constructed by \autoref{eq:obs-gen}\footnote{
\textcite{douDiffusionPosteriorSampling2023} considers a slightly different construction which
employs ``noise-sharing'' with the state's forward-process; see \ref{sec:extra}.} and $g$ as in
\autoref{eq:obs-likelihood}. Plugging this in to \autoref{eq:weight-func-gen}, we get
\begin{align*}
    \omega_t &= \int p_\theta(\tilde{\mathbf{x}}_t \mid \mathbf{x}_{t+1})g(\mathbf{y}_{t} \mid \tilde{\mathbf{x}}_{t})\, d\mathbf{x}_t
    = g(\mathbf{y}_t \mid \mathbf{x}_{t+1}) \\
    &= \mathcal{N}\left(\mathbf{y}_t; A\mu_{t+1}(\mathbf{x}_{t+1}, \theta), \Xi_t + A\Sigma_{t+1}A^\top\right)
\end{align*}
where we apply \autoref{prop:gaussian-exact}. Note this \emph{only} holds when we have a linear inverse
problem; otherwise $g(\mathbf{y}_t \mid \mathbf{x}_{t+1})$ is intractable. In the case of FPS-SMC,
these are exactly the weights used in the SMC procedure; in the case of MCGDiff they similarly
use the previous likelihood, $g(\mathbf{y}_{t+1} \mid \mathbf{x}_{t+1})$, as an auxiliary variable.
Through this lens, we may view MCGDiff as a special case of \autoref{alg:smc-opt} where we choose
\begin{align*}
    f(\tilde{\mathbf{x}}_t) &= g(\mathbf{y}_t \mid \mathbf{x}_{t+1}) = \mathcal{N}\left(\mathbf{y}_t; A\mu_{t+1}(\mathbf{x}_{t+1}, \theta), \Xi_t + A\Sigma_{t+1}A^\top\right)
\end{align*}
but keep the auxiliary variable (abusing notation) as
$f(\mathbf{x}_{t+1}) = g(\mathbf{y}_{t+1} \mid \mathbf{x}_{t+1})$. The interpretation is very
similar to before except rather than more strongly weighting those particles whose likelihood
\emph{increased} relative to their previous position, we more strongly weight those whose whose
likelihood \emph{will increase with the next transition} relative to their previous position more
strongly. The idea is then to properly implement an auxiliary particle filter, and weight particles
\emph{before} moving them. This generally should improve the performance since it's essentially
`forward-looking'. However, it's important to note this can only be applied to \emph{linear
inverse problems} due to the intractability of $g(\mathbf{y}_t \mid \mathbf{x}_{t+1})$ generally;
it similarly cannot be applied for general optimisation tasks.

With that said, there is nothing prohibiting us from using other proposals, such as that of
MCGDiff/FPS-SMC, in \autoref{alg:smc-opt}, re-deriving the weight function for
\autoref{eq:weight-formula-inverse} from \autoref{eq:weight-func-gen} but still using the previous
likelihood auxiliary variable. As mentioned in \autoref{rem:grad-free}, we could consider using
gradients to construct the proposals. \textcite{wuPracticalAsymptoticallyExact2023} consider
$\nabla_{\mathbf{x}_t}\log p_{\mathbf{y} \mid \hat{\mathbf{x}}_0(\mathbf{x}_t, \theta)}$ (with
$\hat{\mathbf{x}}_0(\mathbf{x}_t, \theta)$ as in \autoref{eq:tweedie}) computed using back-propagation
through the network, $s_\theta$, to twist the proposal. \textcite{boysTweedieMomentProjected2023}
consider $\nabla_{\mathbf{x}_t}\hat{\mathbf{x}}_0(\mathbf{x}_t, \theta)$ to choose a more optimal
covariances in $p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)$. Such methods incur additional
computational cost and are, again, restricted to linear inverse problems. In such cases, however,
the added efficiency of the proposal can (significantly) reduce the number of particles, $N$,
mitigating some of this added computational cost.

In the context of general optimisation, an alternative approach considered by
\textcite{kongDiffusionModelsConstrained2024}
is to run a guided diffusion using the gradient of the objective, in order to roughly sample regions
on the data manifold near to the optima, and then run Langevin steps (i.e. MALA) after to yield
better optima. This has the advantage over \texttt{SMCDiffOpt} of yielding an independent sampler
($N=1$), enabling it to be \emph{embarrassingly parallelised}. The downside to this method is its
reliance on the gradient meaning it necessitates training a surrogate model for the objective
function in black-box settings. Of course, the idea of supplementary sampling steps after running
the guided diffusion is trivially applicable to our method if desired to yield better estimates of
the optima.
